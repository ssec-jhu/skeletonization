{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy train images to  \"target_folder\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import cv2\n",
    "import shutil\n",
    "\n",
    "# Specify the directories\n",
    "current_folder = 'Users/vkluzner/OneDrive/NeuralMorphology/Simulations_16bit_Size3334/images/'\n",
    "target_folder = os.path.join(current_folder, '../TrainingKit/img_train_shape/')\n",
    "\n",
    "# Ensure the destination folder eyists\n",
    "os.makedirs(target_folder, eyist_ok=True)\n",
    "keyword = \"Realistic-SBR-\"\n",
    "\n",
    "# List all files in the current folder that contain the keyword\n",
    "files_with_keyword = [\n",
    "    f for f in os.listdir(current_folder)\n",
    "    if keyword in f and os.path.isfile(os.path.join(current_folder, f))\n",
    "]\n",
    "files_with_keyword = sorted(files_with_keyword)\n",
    "print(files_with_keyword)\n",
    "\n",
    "# Loop through files and organize them\n",
    "for file_name in files_with_keyword:\n",
    "    full_file_name = os.path.join(current_folder, file_name)\n",
    "    \n",
    "    # Drop keyword from the name and change the folder\n",
    "    new_full_file_name = os.path.join(target_folder, file_name.replace(keyword, \"\"))\n",
    "    \n",
    "    image = cv2.imread(full_file_name, cv2.IMREAD_UNCHANGED)  # or IMREAD_COLOR if 3 channels\n",
    "    # Put the image with sizes devidable by 16 in the training location\n",
    "    if image.shape[0] % 16 == 0 and image.shape[1] % 16 == 0: # just copy the same image to the training location\n",
    "        shutil.copy(full_file_name, new_full_file_name)\n",
    "    else:\n",
    "        pad_height = math.ceil(image.shape[0] / 16) * 16 - image.shape[0]\n",
    "        pad_width = math.ceil(image.shape[1] / 16) * 16 - image.shape[1]\n",
    "        # Pad image size to be devidable by 16\n",
    "        border_type = cv2.BORDER_REPLICATE\n",
    "        padded_image = cv2.copyMakeBorder(image, math.floor(pad_height/2), math.ceil(pad_height/2), math.floor(pad_width/2), math.ceil(pad_width/2), border_type)\n",
    "        cv2.imwrite(new_full_file_name, padded_image)\n",
    "\n",
    "    print(new_full_file_name)\n",
    "    \n",
    "print(\"Copy to img_train_shape folder complete.\")\n",
    "\n",
    "\n",
    "target_folder = os.path.join(current_folder, '../TrainingKit/img_train2/')\n",
    "# Ensure the destination folder eyists\n",
    "os.makedirs(target_folder, eyist_ok=True)\n",
    "keyword = \"Skeleton\"\n",
    "\n",
    "# List all files in the current folder that contain the keyword\n",
    "files_with_keyword = [\n",
    "    f for f in os.listdir(current_folder)\n",
    "    if keyword in f and os.path.isfile(os.path.join(current_folder, f))\n",
    "]\n",
    "files_with_keyword = sorted(files_with_keyword)\n",
    "print(files_with_keyword)\n",
    "\n",
    "# Loop through files and organize them\n",
    "for file_name in files_with_keyword:\n",
    "    full_file_name = os.path.join(current_folder, file_name)\n",
    "    # Drop keyword from the name and change the folder\n",
    "    for cntr in range(1, 6):\n",
    "        new_full_file_name = os.path.join(target_folder, file_name.replace(keyword, str(cntr)))\n",
    "        \n",
    "        image = cv2.imread(full_file_name, cv2.IMREAD_UNCHANGED)  # or IMREAD_COLOR if 3 channels\n",
    "        # Put the image with sizes devidable by 16 in the training location\n",
    "        if image.shape[0] % 16 == 0 and image.shape[1] % 16 == 0: # just copy the same image to the training location\n",
    "            shutil.copy(full_file_name, new_full_file_name)\n",
    "        else:\n",
    "            pad_height = math.ceil(image.shape[0] / 16) * 16 - image.shape[0]\n",
    "            pad_width = math.ceil(image.shape[1] / 16) * 16 - image.shape[1]\n",
    "            # Pad image size to be devidable by 16\n",
    "            border_type = cv2.BORDER_REPLICATE\n",
    "            padded_image = cv2.copyMakeBorder(image, math.floor(pad_height/2), math.ceil(pad_height/2), math.floor(pad_width/2), math.ceil(pad_width/2), border_type)\n",
    "            cv2.imwrite(new_full_file_name, padded_image)\n",
    "        \n",
    "        print(new_full_file_name)\n",
    "    \n",
    "print(\"Copy to img_train2 folder complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# Specify the directory\n",
    "folder_path = target_folder\n",
    "pickle_file = '../dataloader/annotation/dataset5.pkl'\n",
    "\n",
    "# Initialize lists for train and val files\n",
    "train_files = []\n",
    "val_files = []\n",
    "\n",
    "# Loop through files and organize them\n",
    "for file_name in os.listdir(folder_path):\n",
    "    # Define criteria for splitting, e.g., keyword or random split\n",
    "    if random.random() < 0.75:  # Example criteria\n",
    "        train_files.append(file_name)\n",
    "    else:\n",
    "        val_files.append(file_name)\n",
    "\n",
    "# Create dictionary structure\n",
    "data = {'train': train_files, 'val': val_files}\n",
    "\n",
    "# Save to a .pkl file\n",
    "with open(pickle_file, 'wb') as pkl_file:\n",
    "    pickle.dump(data, pkl_file)\n",
    "\n",
    "print(\"Data saved to dataloader/annotation/dataset5.pkl\")\n",
    "\n",
    "with open(pickle_file, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "# Inspect the structure\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy train images to  \"target_folder\" while tiling them by TxT\n",
    "\n",
    "import os\n",
    "import math\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Create tiling coordinates\n",
    "T = 512\n",
    "Im_x = 3334\n",
    "Im_y = 3334\n",
    "\n",
    "n_x = math.ceil(Im_x / T)\n",
    "X_coord = np.zeros(n_x, dtype=int)\n",
    "gap_x = math.floor((T * n_x - Im_x) / (n_x - 1))\n",
    "gap_x_plus_one__amount = T * n_x - Im_x - gap_x * (n_x - 1)\n",
    "for i in range(1, n_x):\n",
    "    if i <= gap_x_plus_one__amount:\n",
    "        X_coord[i] = int(X_coord[i-1] + T - (gap_x + 1))\n",
    "    else:\n",
    "        X_coord[i] = int(X_coord[i-1] + T - gap_x)\n",
    "    \n",
    "n_y = math.ceil(Im_y / T)\n",
    "Y_coord = np.zeros(n_y, dtype=int)\n",
    "gap_y = math.floor((T * n_y - Im_y) / (n_y - 1))\n",
    "gap_y_plus_one__amount = T * n_y - Im_y - gap_y * (n_y - 1)\n",
    "for i in range(1, n_y):\n",
    "    if i <= gap_y_plus_one__amount:\n",
    "        Y_coord[i] = int(Y_coord[i-1] + T - (gap_y + 1))\n",
    "    else:\n",
    "        Y_coord[i] = int(Y_coord[i-1] + T - gap_y)\n",
    "\n",
    "\n",
    "# Specify the directories\n",
    "current_folder = '/home/idies/workspace/ssec_neural_morphology/Simulations_16bit_Size3334/images/'\n",
    "#current_folder = 'Users/vkluzner//OneDrive/NeuralMorphology/Simulations_16bit_Size3334/images/'\n",
    "target_folder = os.path.join(current_folder, '../TrainingKit/img_train_shape/')\n",
    "\n",
    "# Ensure the destination folder exists\n",
    "os.makedirs(target_folder, exist_ok=True)\n",
    "keyword = \"Realistic-SBR-\"\n",
    "\n",
    "# List all files in the current folder that contain the keyword\n",
    "files_with_keyword = [\n",
    "    f for f in os.listdir(current_folder)\n",
    "    if keyword in f and os.path.isfile(os.path.join(current_folder, f))\n",
    "]\n",
    "files_with_keyword = sorted(files_with_keyword)\n",
    "print(files_with_keyword)\n",
    "\n",
    "# Loop through files and create tiles\n",
    "for file_name in files_with_keyword:\n",
    "    full_file_name = os.path.join(current_folder, file_name)\n",
    "    \n",
    "    # Drop keyword from the name and change the folder\n",
    "    new_full_file_name = os.path.join(target_folder, file_name.replace(keyword, \"\"))\n",
    "    image = cv2.imread(full_file_name, cv2.IMREAD_UNCHANGED)  # or IMREAD_COLOR if 3 channels\n",
    "    for i in range(n_y):\n",
    "        for j in range(n_x):\n",
    "            new_full_file_name_tile = new_full_file_name.replace(\".pgm\", \"_tile_\" + str(i) + \"_\" + str(j) + \".pgm\")\n",
    "            tile = image[Y_coord[i]:(Y_coord[i] + T), X_coord[j]:(X_coord[j] + T)] # Crop the ROI\n",
    "            cv2.imwrite(new_full_file_name_tile, tile)\n",
    "            print(new_full_file_name_tile)\n",
    "    \n",
    "print(\"Copy to img_train_shape folder complete.\")\n",
    "\n",
    "\n",
    "target_folder = os.path.join(current_folder, '../TrainingKit/img_train2/')\n",
    "# Ensure the destination folder exists\n",
    "os.makedirs(target_folder, exist_ok=True)\n",
    "keyword = \"Skeleton\"\n",
    "\n",
    "# List all files in the current folder that contain the keyword\n",
    "files_with_keyword = [\n",
    "    f for f in os.listdir(current_folder)\n",
    "    if keyword in f and os.path.isfile(os.path.join(current_folder, f))\n",
    "]\n",
    "files_with_keyword = sorted(files_with_keyword)\n",
    "print(files_with_keyword)\n",
    "\n",
    "# Loop through files and create tiles\n",
    "for file_name in files_with_keyword:\n",
    "    full_file_name = os.path.join(current_folder, file_name)\n",
    "    \n",
    "    # Change keyword by the number and change the folder\n",
    "    for cntr in range(1, 6):\n",
    "        new_full_file_name = os.path.join(target_folder, file_name.replace(keyword, str(cntr)))\n",
    "        image = cv2.imread(full_file_name, cv2.IMREAD_UNCHANGED)  # or IMREAD_COLOR if 3 channels\n",
    "        for i in range(n_y):\n",
    "            for j in range(n_x):\n",
    "                new_full_file_name_tile = new_full_file_name.replace(\".pgm\", \"_tile_\" + str(i) + \"_\" + str(j) + \".pgm\")\n",
    "                tile = image[Y_coord[i]:(Y_coord[i] + T), X_coord[j]:(X_coord[j] + T)] # Crop the ROI\n",
    "                cv2.imwrite(new_full_file_name_tile, tile)\n",
    "                print(new_full_file_name_tile)\n",
    "    \n",
    "print(\"Copy to img_train2 folder complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "pickle_file = '../dataloader/annotation/dataset5.pkl'\n",
    "with open(pickle_file, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "# Extract train and val files\n",
    "train_files = data['train']\n",
    "val_files = data['val']\n",
    "   \n",
    "# Initialize lists for train and val files\n",
    "new_train_files = []\n",
    "new_val_files = []\n",
    "\n",
    "# Loop through files and organize them\n",
    "n_x = 7\n",
    "n_y = n_x\n",
    "for file_name in train_files:\n",
    "    for i in range(n_y):\n",
    "        for j in range(n_x):\n",
    "            new_train_files.append(file_name.replace(\".pgm\", \"_tile_\" + str(i) + \"_\" + str(j) + \".pgm\"))\n",
    "            num_selected = int(0.08 * len(new_train_files))  # Calculate 8% of the total files\n",
    "            selected_train_files = random.sample(new_train_files, num_selected)  # Randomly select 8%\n",
    "            \n",
    "for file_name in val_files:\n",
    "    for i in range(n_y):\n",
    "        for j in range(n_x):\n",
    "            new_val_files.append(file_name.replace(\".pgm\", \"_tile_\" + str(i) + \"_\" + str(j) + \".pgm\"))\n",
    "            new_val_files.sort()\n",
    "            \n",
    "# Create dictionary structure\n",
    "new_data = {'train': selected_train_files, 'val': new_val_files}\n",
    "\n",
    "# Save to a .pkl file\n",
    "new_pickle_file = '../dataloader/annotation/dataset6.pkl'\n",
    "with open(new_pickle_file, 'wb') as pkl_file:\n",
    "    pickle.dump(new_data, pkl_file)\n",
    "\n",
    "print(\"Data saved to dataloader/annotation/dataset6.pkl\")\n",
    "\n",
    "with open(new_pickle_file, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "# Inspect the structure\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pickle file for test purposes\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Specify the directory\n",
    "pickle_file = '../dataloader/annotation/test.pkl'\n",
    "\n",
    "# Initialize lists for train and val files\n",
    "train_files = ['1-Sample-1-time-36.00.pgm']\n",
    "val_files = ['1-Sample-2-time-36.00.pgm']\n",
    "\n",
    "# Create dictionary structure\n",
    "data = {'train': train_files, 'val': val_files}\n",
    "\n",
    "# Save to a .pkl file\n",
    "with open(pickle_file, 'wb') as pkl_file:\n",
    "    pickle.dump(data, pkl_file)\n",
    "\n",
    "print(\"Data saved to \", pickle_file)\n",
    "\n",
    "with open(pickle_file, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "# Inspect the structure\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert greylevel pgm files from 16 bit to 8 bit, resize it up to 1667x1667 and save it as png files\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Specify the directory\n",
    "current_folder = '../dataset/Starting Kit Pixel/img_train_shape'\n",
    "\n",
    "# Loop through files and organize them\n",
    "for file_name in os.listdir(current_folder):\n",
    "    if file_name.endswith('.pgm'):  # Adjust as needed for other extensions\n",
    "        # Step 1: Read the 16-bit PGM image\n",
    "        full_file_name = os.path.join(current_folder, file_name)\n",
    "        image_16bit = cv2.imread(full_file_name, cv2.IMREAD_UNCHANGED)  # Load as-is for 16-bit\n",
    "\n",
    "        # Step 2: Normalize to 8-bit\n",
    "        image_8bit_temp = cv2.normalize(image_16bit, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "        image_8bit = cv2.equalizeHist(image_8bit_temp)\n",
    "        \n",
    "        # Step 3: Resize to 1667x1667 pixels\n",
    "        image_resized = cv2.resize(image_8bit, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # Step 4: Save as 8-bit PNG\n",
    "        new_full_file_name = os.path.splitext(full_file_name)[0] + \".png\"\n",
    "        cv2.imwrite(new_full_file_name, image_resized)\n",
    "\n",
    "print(\"Conversion and resizing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert binary pgm files from 16 bit to 8 bit, resize it up to 1667x1667 and save it as png files\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Specify the directory\n",
    "current_folder = '../dataset/Starting Kit Pixel/img_train2'\n",
    "\n",
    "# Loop through files and organize them\n",
    "for file_name in os.listdir(current_folder):\n",
    "    if file_name.endswith('.pgm'):  # Adjust as needed for other extensions\n",
    "        # Step 1: Read the 16-bit PGM image\n",
    "        full_file_name = os.path.join(current_folder, file_name)\n",
    "        image_16bit = cv2.imread(full_file_name, cv2.IMREAD_UNCHANGED)  # Load as-is for 16-bit\n",
    "\n",
    "        # Step 2: Normalize to 8-bit\n",
    "        image_8bit = cv2.normalize(image_16bit, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "        \n",
    "        # Step 3: Resize to 1667x1667 pixels\n",
    "        image_resized = cv2.resize(image_8bit, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # Step 4: Binarize\n",
    "        _, binary_image = cv2.threshold(image_resized, thresh=1, maxval=255, type=cv2.THRESH_BINARY)\n",
    "        \n",
    "        # Step 5: Save as 8-bit PNG\n",
    "        new_full_file_name = os.path.splitext(full_file_name)[0] + \".png\"\n",
    "        cv2.imwrite(new_full_file_name, binary_image)\n",
    "\n",
    "print(\"Conversion and resizing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize greylevel pgm files from 1000x1000 to 1024x1024 and save it as png files\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Specify the directory\n",
    "current_folder = '../dataset/Starting Kit Pixel2/img_train_shape'\n",
    "image = cv2.imread('../dataset/Starting Kit Pixel2/img_train_shape/Sample-1-time-36.00.pgm', cv2.IMREAD_UNCHANGED)  # Load as-is\n",
    "height, width = image.shape\n",
    "# Create a new array with padding, initialized to black (0)\n",
    "template_image = np.zeros((height+24, width+24), dtype=image.dtype)\n",
    "template_image[0:12, 12:12+width] = image[0:12, 0:width]\n",
    "template_image[height+12:height+24, 12:12+width] = image[height-12:height, 0:width]\n",
    "template_image[12:12+height, 0:12] = image[0:height, 0:12]\n",
    "template_image[12:12+height, width+12:width+24] = image[0:height, width-12:width]\n",
    "template_image[0:12, 0:12] = image[height-12:height, width-12:width]\n",
    "template_image[0:12, width+12:width+24] = image[height-12:height, 0:12]\n",
    "template_image[height+12:height+24, 0:12] = image[0:12, width-12:width]\n",
    "template_image[height+12:height+24, width+12:width+24] = image[0:12, 0:12]\n",
    "     \n",
    "# Loop through files and organize them\n",
    "for file_name in os.listdir(current_folder):\n",
    "    if file_name.endswith('.pgm'):  # Adjust as needed for other extensions\n",
    "        # Step 1: Read the PGM image\n",
    "        full_file_name = os.path.join(current_folder, file_name)\n",
    "        orig_image = cv2.imread(full_file_name, cv2.IMREAD_UNCHANGED)  # Load as-is\n",
    "\n",
    "        # Get the dimensions of the original image\n",
    "        height, width = orig_image.shape\n",
    "\n",
    "        # Create a new array with padding, initialized to black (0)\n",
    "        padded_image = template_image\n",
    "        padded_image[12:12+height, 12:12+width] = orig_image\n",
    "\n",
    "        # Step 4: Save as 8-bit PNG\n",
    "        new_full_file_name = os.path.splitext(full_file_name)[0] + \".png\"\n",
    "        cv2.imwrite(new_full_file_name, padded_image)\n",
    "\n",
    "print(\"Conversion and resizing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize binary pgm files from 1000x1000 to 1024x1024 and save it as png files\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Specify the directory\n",
    "current_folder = '../dataset/Starting Kit Pixel2/img_train2'\n",
    "image = cv2.imread('../dataset/Starting Kit Pixel2/img_train2/Sample-1-time-36.00.pgm', cv2.IMREAD_UNCHANGED)  # Load as-is\n",
    "height, width = image.shape\n",
    "# Create a new array with padding, initialized to black (0)\n",
    "template_image = np.zeros((height+24, width+24), dtype=image.dtype)\n",
    "     \n",
    "# Loop through files and organize them\n",
    "for file_name in os.listdir(current_folder):\n",
    "    if file_name.endswith('.pgm'):  # Adjust as needed for other extensions\n",
    "        # Step 1: Read the PGM image\n",
    "        full_file_name = os.path.join(current_folder, file_name)\n",
    "        orig_image = cv2.imread(full_file_name, cv2.IMREAD_UNCHANGED)  # Load as-is\n",
    "\n",
    "        # Get the dimensions of the original image\n",
    "        height, width = orig_image.shape\n",
    "\n",
    "        # Create a new array with padding, initialized to black (0)\n",
    "        padded_image = template_image\n",
    "        padded_image[12:12+height, 12:12+width] = orig_image\n",
    "\n",
    "        # Step 4: Save as 8-bit PNG\n",
    "        new_full_file_name = os.path.splitext(full_file_name)[0] + \".png\"\n",
    "        cv2.imwrite(new_full_file_name, padded_image)\n",
    "\n",
    "print(\"Conversion and resizing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../dataloader/annotation/dataset6.pkl', 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "values = data.get('train') + data.get('val')\n",
    "#values.sort()\n",
    "print(sorted(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "# Specify the directory\n",
    "pickle_file = '../dataloader/annotation/dataset7.pkl'\n",
    "\n",
    "filenames = []\n",
    "for sbr in range(1, 6):\n",
    "    for cntr in range(1, 101):\n",
    "        file_name = str(sbr) + \"-Sample-\" + str(cntr) + \"-time-100.00_tile_\"\n",
    "        filenames.append(file_name)\n",
    "        \n",
    "# Initialize lists for train and val files\n",
    "train_files = []\n",
    "val_files = []\n",
    "\n",
    "# Loop through files and organize them\n",
    "for file_name in filenames:\n",
    "    # Define criteria for splitting, e.g., keyword or random split\n",
    "    if random.random() < 0.75:  # Example criteria\n",
    "        for i in range(7): #(1, 6): #(7):\n",
    "            for j in range(7): #(1, 6): #(7):\n",
    "                train_files.append(file_name + str(i) + \"_\" + str(j) + \".pgm\")\n",
    "    else:\n",
    "        for i in range(7):\n",
    "            for j in range(7):\n",
    "                val_files.append(file_name + str(i) + \"_\" + str(j) + \".pgm\")\n",
    "\n",
    "# Create dictionary structure\n",
    "data = {'train': train_files, 'val': val_files}\n",
    "\n",
    "# Save to a .pkl file\n",
    "with open(pickle_file, 'wb') as pkl_file:\n",
    "    pickle.dump(data, pkl_file)\n",
    "\n",
    "print(\"Data saved to dataset pkl file\")\n",
    "\n",
    "with open(pickle_file, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "# Inspect the structure\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "pickle_file = '../dataloader/annotation/dataset7.pkl'\n",
    "\n",
    "# Load data\n",
    "with open(pickle_file, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "# Pad the shorter list with None to ensure equal length\n",
    "max_len = max(len(data['train']), len(data['val']))\n",
    "train_padded = data['train'] + [None] * (max_len - len(data['train']))\n",
    "val_padded = data['val'] + [None] * (max_len - len(data['val']))\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'train': train_padded,\n",
    "    'val': val_padded\n",
    "})\n",
    "\n",
    "# Pad the shorter list with None to ensure equal length\n",
    "max_len = max(len(data['train']), len(data['val']))\n",
    "train_padded = data['train'] + [None] * (max_len - len(data['train']))\n",
    "val_padded = data['val'] + [None] * (max_len - len(data['val']))\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'train': train_padded,\n",
    "    'val': val_padded\n",
    "})\n",
    "\n",
    "output_csv = 'dataset7_split.csv'\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"Data saved to {output_csv}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skelneton",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
