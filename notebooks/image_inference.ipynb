{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Trained U-Net Model\n",
    "# •\tMake sure your model is saved and accessible. For PyTorch load it with torch.load(), and for TensorFlow/Keras use keras.models.load_model()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# Get the current working directory\n",
    "current_path = os.getcwd()\n",
    "print(\"Current Working Directory:\", current_path)\n",
    "# Get the parent directory\n",
    "parent_path = os.path.dirname(current_path)\n",
    "print(\"Parent Directory:\", parent_path)\n",
    "\n",
    "#sys.path.append(current_path)\n",
    "sys.path.append(parent_path)\n",
    "\n",
    "# Print the updated sys.path\n",
    "print(\"Updated sys.path:\")\n",
    "for p in sys.path:\n",
    "    print(p)\n",
    "\n",
    "# Find available GPU\n",
    "if torch.backends.mps.is_available(): # Check if PyTorch has access to MPS (Metal Performance Shader, Apple's GPU architecture)\n",
    "    print(\"MPS is available!\")\n",
    "    if torch.backends.mps.is_built():\n",
    "        print(\"MPS (Metal Performance Shader) is built in!\")    \n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available(): # Check if PyTorch has access to CUDA (Win or Linux's GPU architecture)\n",
    "    print(\"CUDA is available!\")\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\"Only CPU is available!\")\n",
    "    device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "#check out kernel\n",
    "print(\"Python executable:\", sys.executable)\n",
    "print(\"Python version:\", sys.version)\n",
    "\n",
    "from model.unet_att import UnetAttention\n",
    "model = UnetAttention()\n",
    "model = model.to(device)\n",
    "ckpt = torch.load(\"../../../../OneDrive/NeuralMorphology/Simulations_16bit_Size3334/output//ex2/ckpt.pth\", weights_only=False, map_location=device)\n",
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "model.eval()  # Set to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show differences on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load image\n",
    "image_16bit = cv2.imread(\"../../../../OneDrive/NeuralMorphology/CIV_Developmental_Images/24Hr/Processed/Oriented/24hrWt2_40x_20gfp_500ms_482z_frmlefttopA3A4A5001-MaxIP-A4-Ori.tif\", cv2.IMREAD_UNCHANGED)  # or IMREAD_COLOR if 3 channels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Input Image\n",
    "#\t•\tEnsure the image has the same dimensions, channels, and scaling as used during training. U-Net models usually work with 2D images with dimensions like 256x256 or 512x512 pixels.\n",
    "#\t•\tNormalize or scale the pixel values if needed (e.g., dividing by 255 for images in the 0-255 range)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load image\n",
    "image_16bit = cv2.imread(\"../../../../OneDrive/NeuralMorphology/CIV_Developmental_Images/24Hr/Processed/Oriented/24hrWt2_40x_20gfp_500ms_482z_frmlefttopA3A4A5001-MaxIP-A4-Ori.tif\", cv2.IMREAD_UNCHANGED)  # or IMREAD_COLOR if 3 channels\n",
    "\n",
    "# Step 2: Normalize to 8-bit\n",
    "image = cv2.normalize(image_16bit, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "height, width = image.shape\n",
    "   \n",
    "# Calculate the square size of the padded image\n",
    "pwr = math.ceil(max(math.log2(height), math.log2(width)))\n",
    "side = 2 ** pwr\n",
    "\n",
    "# Calculate start coordinates for the original image\n",
    "start_height = math.floor((side - height) / 2)\n",
    "start_width = math.floor((side - width) / 2)\n",
    "\n",
    "# Implement Gaussian Mixture for Background Analysis:\n",
    "\n",
    "# Step 2: Flatten the image\n",
    "pixels = image.flatten().reshape(-1, 1)  # Reshape to (n_samples, 1)\n",
    "\n",
    "# Step 3: Fit Gaussian Mixture Model\n",
    "gmm = GaussianMixture(n_components=2, random_state=42)\n",
    "gmm.fit(pixels)\n",
    "\n",
    "# Step 4: Analyze GMM components\n",
    "labels = gmm.predict(pixels)  # Labels for each pixel\n",
    "means = gmm.means_.flatten()\n",
    "variances = gmm.covariances_.flatten()\n",
    "\n",
    "# Identify background component (typically the one with the lowest mean intensity)\n",
    "background_label = np.argmin(means)\n",
    "print(\"Background Label:\", background_label)\n",
    "print(\"Background Mean Intensity:\", means[background_label])\n",
    "\n",
    "# Step 5: Reshape the labels to the original image shape\n",
    "segmented_image = labels.reshape(image.shape)\n",
    "\n",
    "# Visualize the segmentation\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Original Image\")\n",
    "plt.imshow(image, cmap='gray')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Segmented Background\")\n",
    "plt.imshow(segmented_image == background_label, cmap='gray')  # Highlight background\n",
    "plt.show()\n",
    "\n",
    "# Create a new array with padding, initialized with background as Gaussian variable\n",
    "# padded_image = np.zeros((side, side), dtype=image.dtype)\n",
    "# padded_image[0:side, 0:start_width] = padding_gray_level\n",
    "# padded_image[0:side, start_width+width:side] = padding_gray_level\n",
    "# padded_image[0:start_height, start_width:start_width+width] = padding_gray_level\n",
    "# padded_image[start_height+height:side, start_width:start_width+width] = padding_gray_level\n",
    "\n",
    "# Step 3: Generate Gaussian-distributed pixel values\n",
    "# Use np.random.normal to generate values for each pixel in the image\n",
    "padded_image = np.random.normal(loc=means[background_label], scale=np.sqrt(variances[background_label]), size=(side, side))\n",
    "\n",
    "# Step 4: Clip the values to the valid pixel range [0, 255] for an 8-bit grayscale image\n",
    "padded_image = np.clip(padded_image, 0, 255)\n",
    "\n",
    "# Step 5: Convert to integer values (uint8)\n",
    "padded_image = padded_image.astype(np.uint8)\n",
    "\n",
    "padded_image[start_height:start_height+height, start_width:start_width+width] = image\n",
    "cv2.imwrite(\"orig.png\", padded_image)\n",
    "\n",
    "padded_image = padded_image / 255.\n",
    "image_flip_0 = cv2.flip(padded_image, 0)\n",
    "image_flip_1 = cv2.flip(padded_image, 1)\n",
    "image_flip__1 = cv2.flip(padded_image, -1)\n",
    "image = np.stack([padded_image, image_flip_0, image_flip_1, image_flip__1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load image\n",
    "image = cv2.imread(\"../dataset/test/Realistic-SBR-1-Sample-1-time-36.00.pgm\", cv2.IMREAD_UNCHANGED)  # or IMREAD_COLOR if 3 channels\n",
    "height, width = image.shape\n",
    "# Create a new array with padding, initialized to black (0)\n",
    "template_image = np.zeros((height+24, width+24), dtype=image.dtype)\n",
    "template_image[0:12, 12:12+width] = image[0:12, 0:width]\n",
    "template_image[height+12:height+24, 12:12+width] = image[height-12:height, 0:width]\n",
    "template_image[12:12+height, 0:12] = image[0:height, 0:12]\n",
    "template_image[12:12+height, width+12:width+24] = image[0:height, width-12:width]\n",
    "template_image[0:12, 0:12] = image[height-12:height, width-12:width]\n",
    "template_image[0:12, width+12:width+24] = image[height-12:height, 0:12]\n",
    "template_image[height+12:height+24, 0:12] = image[0:12, width-12:width]\n",
    "template_image[height+12:height+24, width+12:width+24] = image[0:12, 0:12]\n",
    "padded_image = template_image\n",
    "padded_image[12:12+height, 12:12+width] = image\n",
    "cv2.imwrite(\"orig.png\", padded_image)\n",
    "\n",
    "padded_image = padded_image / 255.\n",
    "image_flip_0 = cv2.flip(padded_image, 0)\n",
    "image_flip_1 = cv2.flip(padded_image, 1)\n",
    "image_flip__1 = cv2.flip(padded_image, -1)\n",
    "image = np.stack([padded_image, image_flip_0, image_flip_1, image_flip__1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Inference\n",
    "#\t•\tPass the prepared image through the U-Net model to obtain the predicted segmentation mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch example\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    input_tensor = torch.tensor(image).unsqueeze(1).to(torch.float32).to('mps')\n",
    "    pred, _, _, _ = model(input_tensor)\n",
    "    pred = torch.sigmoid(pred)\n",
    "    pred_ori, pred_flip_0, pred_flip_1, pred_flip__1 = pred\n",
    "    pred_ori = pred_ori.cpu().numpy()\n",
    "    pred_flip_0 = cv2.flip(pred_flip_0.cpu().numpy(), 0)\n",
    "    pred_flip_1 = cv2.flip(pred_flip_1.cpu().numpy(), 1)\n",
    "    pred_flip__1 = cv2.flip(pred_flip__1.cpu().numpy(), -1)\n",
    "    pred = np.mean([pred_ori, pred_flip_0, pred_flip_1, pred_flip__1], axis=0)\n",
    "    pred = pred[start_height:start_height+height, start_width:start_width+width]\n",
    "    \n",
    "threshold = 0.79\n",
    "binary_mask = (pred > threshold).astype(np.uint8) * 255  # Convert to 0 and 255 for binary mask\n",
    "# Save the mask\n",
    "cv2.imwrite(\"predicted_mask2.png\", binary_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display or Save the Inference Result\n",
    "#\t•\tYou can save or display the binary mask as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('Output Window', binary_mask)\n",
    "# Wait for a key press indefinitely or for a specific amount of time\n",
    "cv2.waitKey(0)  # Waits indefinitely; use cv2.waitKey(1000) to wait 1 second\n",
    "# Destroy all OpenCV windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze inference results for CC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "folder_path = '/Users/vkluzner/OneDrive/NeuralMorphology/Simulations_16bit_Size1024/output/ex7/evaluation/'\n",
    "file_list = glob.glob(os.path.join(folder_path, '*_pred_bin.tif'))\n",
    "print(\"Files with '_pred_bin.tif':\")\n",
    "for file in file_list:\n",
    "    image = cv2.imread(os.path.join(folder_path, file), cv2.IMREAD_UNCHANGED)\n",
    "    _, binary_image = cv2.threshold(image, 127, 1, cv2.THRESH_BINARY)\n",
    "    #num_labels, labeled_image = cv2.connectedComponents(binary_image)\n",
    "    #num_cc = num_labels - 1 # Subtract 1 to exclude the background component\n",
    "    num_labels, labeled_image, stats, centroids = cv2.connectedComponentsWithStats(binary_image, connectivity=8)\n",
    "    num_connected_components = num_labels - 1\n",
    "    num_cc = num_labels - 1 # Subtract 1 to exclude the background component\n",
    "    if num_cc > 1:\n",
    "            print(f'Number of connected components for {file} is: {num_cc}')\n",
    "    \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer image through tiling process\n",
    "\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "# Get the current working directory\n",
    "current_path = os.getcwd()\n",
    "print(\"Current Working Directory:\", current_path)\n",
    "# Get the parent directory\n",
    "parent_path = os.path.dirname(current_path)\n",
    "print(\"Parent Directory:\", parent_path)\n",
    "\n",
    "#sys.path.append(current_path)\n",
    "sys.path.append(parent_path)\n",
    "\n",
    "# Print the updated sys.path\n",
    "print(\"Updated sys.path:\")\n",
    "for p in sys.path:\n",
    "    print(p)\n",
    "\n",
    "# Find available GPU\n",
    "if torch.backends.mps.is_available(): # Check if PyTorch has access to MPS (Metal Performance Shader, Apple's GPU architecture)\n",
    "    print(\"MPS is available!\")\n",
    "    if torch.backends.mps.is_built():\n",
    "        print(\"MPS (Metal Performance Shader) is built in!\")    \n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available(): # Check if PyTorch has access to CUDA (Win or Linux's GPU architecture)\n",
    "    print(\"CUDA is available!\")\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\"Only CPU is available!\")\n",
    "    device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "#check out kernel\n",
    "print(\"Python executable:\", sys.executable)\n",
    "print(\"Python version:\", sys.version)\n",
    "\n",
    "from model.unet_att import UnetAttention\n",
    "model = UnetAttention()\n",
    "model = model.to(device)\n",
    "ckpt = torch.load(\"../../../../OneDrive/NeuralMorphology/Simulations_16bit_Size3334/output/ex1/ckpt.pth\", weights_only=False, map_location=device)\n",
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Load image\n",
    "image = cv2.imread(\"../../../../OneDrive/NeuralMorphology/Simulations_16bit_Size3334/images/Realistic-SBR-1-Sample-1-time-100.00.pgm\", cv2.IMREAD_UNCHANGED)  # or IMREAD_COLOR if 3 channels\n",
    "image = cv2.convertScaleAbs(image, alpha=255.0 / image.max()) / 255.\n",
    "Im_y, Im_x = image.shape\n",
    "\n",
    "T = 512 # Tile size\n",
    "\n",
    "# Create tiling coordinates\n",
    "n_x = math.ceil(Im_x / T)\n",
    "X_coord = np.zeros(n_x, dtype=int)\n",
    "gap_x = math.floor((T * n_x - Im_x) / (n_x - 1))\n",
    "gap_x_plus_one__amount = T * n_x - Im_x - gap_x * (n_x - 1)\n",
    "for i in range(1, n_x):\n",
    "    if i <= gap_x_plus_one__amount:\n",
    "        X_coord[i] = int(X_coord[i-1] + T - (gap_x + 1))\n",
    "    else:\n",
    "        X_coord[i] = int(X_coord[i-1] + T - gap_x)\n",
    "    \n",
    "n_y = math.ceil(Im_y / T)\n",
    "Y_coord = np.zeros(n_y, dtype=int)\n",
    "gap_y = math.floor((T * n_y - Im_y) / (n_y - 1))\n",
    "gap_y_plus_one__amount = T * n_y - Im_y - gap_y * (n_y - 1)\n",
    "for i in range(1, n_y):\n",
    "    if i <= gap_y_plus_one__amount:\n",
    "        Y_coord[i] = int(Y_coord[i-1] + T - (gap_y + 1))\n",
    "    else:\n",
    "        Y_coord[i] = int(Y_coord[i-1] + T - gap_y)\n",
    "\n",
    "pred_array = np.zeros((n_x * n_y, Im_y, Im_x), dtype=np.float32)\n",
    "for i in range(n_y):\n",
    "    for j in range(n_x):\n",
    "        tile = image[Y_coord[i]:(Y_coord[i] + T), X_coord[j]:(X_coord[j] + T)] # Crop the ROI\n",
    "        print(\"Infering tile: \", i, j)\n",
    "        # Start the infering process\n",
    "        tile_flip_0 = cv2.flip(tile, 0)\n",
    "        tile_flip_1 = cv2.flip(tile, 1)\n",
    "        tile_flip__1 = cv2.flip(tile, -1)\n",
    "        tile_stack = np.stack([tile, tile_flip_0, tile_flip_1, tile_flip__1])\n",
    "        tile_torch = torch.tensor(tile_stack).unsqueeze(1).to(torch.float32).to(device)\n",
    "        with torch.no_grad():\n",
    "            pred, _, _, _ = model(tile_torch)\n",
    "            pred = torch.sigmoid(pred)\n",
    "            pred_ori, pred_flip_0, pred_flip_1, pred_flip__1 = pred\n",
    "        pred_ori = pred_ori.cpu().numpy()\n",
    "        pred_flip_0 = cv2.flip(pred_flip_0.cpu().numpy(), 0)\n",
    "        pred_flip_1 = cv2.flip(pred_flip_1.cpu().numpy(), 1)\n",
    "        pred_flip__1 = cv2.flip(pred_flip__1.cpu().numpy(), -1)\n",
    "        tile_pred = np.mean([pred_ori, pred_flip_0, pred_flip_1, pred_flip__1], axis=0)\n",
    "        pred_array[i * n_x + j, Y_coord[i]:(Y_coord[i] + T), X_coord[j]:(X_coord[j] + T)] += tile_pred\n",
    "        \n",
    "# Averaging the result\n",
    "non_zero_mask = pred_array != 0  # Shape (n_x * n_y, img_height, img_width)\n",
    "non_zero_count = np.sum(non_zero_mask, axis=0)  # Shape (img_height, img_width)\n",
    "non_zero_count[non_zero_count == 0] = 1  # Prevent division by zero\n",
    "non_zero_sum = np.sum(pred_array * non_zero_mask, axis=0)  # Shape (img_height, img_width)\n",
    "average_image = non_zero_sum / non_zero_count  # Shape (img_height, img_width)\n",
    "\n",
    "average_image = (average_image * 255).astype(np.uint8)\n",
    "cv2.imwrite(\"predicted_mask.png\", average_image)\n",
    "\n",
    "#cv2.imshow('Output Window', average_image)\n",
    "#cv2.waitKey(0)  # Waits indefinitely; use cv2.waitKey(1000) to wait 1 second\n",
    "#cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skelneton",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
